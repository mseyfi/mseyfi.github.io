 
**Hi, Iâ€™m Mehdi, and I use this blog to document my learning journey. 
 I am a research scientist at Meta <img src="images/Meta.png"  width="30" height="30"> | Exploring computer vision, computational photography, and Gen-AI ðŸš€**
<!-- -->

<!-- -->

[<img src="images/LinkedIn_logo_initials.png"  width="40" height="40">](https://www.linkedin.com/in/mehdi-seyfi-38189220/)   [<img src="images/googlescholar.png"  width="40" height="40">](https://scholar.google.ca/citations?user=6l0PmOEAAAAJ&hl=en)
<!-- -->

<!-- -->

## [![Latent Diffusion](https://img.shields.io/badge/Latent_Diffusion-grey?style=for-the-badge&logo=github)](posts/StableDiffusion.md)
<div style="background-color:rgba(200, 200, 210, 0.1470588); text-align:left; vertical-align: center; padding:10px 10;">
Stable Diffusion is a powerful generative model that synthesizes high-quality images guided by textual/another modality descriptions. It leverages the strengths of Variational Autoencoders (VAEs) and Denoising Diffusion Probabilistic Models (DDPMs) to produce images efficiently and effectively.

 
_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>

## [![Diffusion Models DDPM](https://img.shields.io/badge/Diffusion_Models-grey?style=for-the-badge&logo=github)](posts/Diffusion.md)
<div style="background-color:rgba(200, 200, 210, 0.1470588); text-align:left; vertical-align: center; padding:10px 10;">
In this guide, we'll provide sample code for training and inference of a diffusion model, specifically focusing on a Denoising Diffusion Probabilistic Model (DDPM). We'll define the structure for the encoder and decoder using a simplified UNet architecture. Each line of code includes inline comments explaining its purpose, along with the tensor shapes.

 
_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>

## [![GLIP](https://img.shields.io/badge/Grounded_Language_Image_Pre_training_(GLIP)-grey?style=for-the-badge&logo=github)](posts/GLIP.md)
<div style="background-color:rgba(200, 200, 210, 0.1470588); text-align:left; vertical-align: center; padding:10px 10;">
GLIP (Grounded Language-Image Pre-training) is a unified model architecture that bridges the gap between vision and language by integrating object detection and phrase grounding tasks. It leverages both visual and textual data to perform object detection conditioned on textual descriptions, enabling the model to recognize objects based on their semantic meanings.


_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>

## [![CLIP](https://img.shields.io/badge/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision_(CLIP)-grey?style=for-the-badge&logo=github)](posts/CLIP.md)
<div style="background-color:rgba(200, 200, 210, 0.1470588); text-align:left; vertical-align: center; padding:10px 10;">
Learning Transferable Visual Models From Natural Language Supervision" is a groundbreaking paper by OpenAI that introduces CLIP (Contrastive Language-Image Pre-training). CLIP learns visual concepts from natural language supervision by jointly training an image encoder and a text encoder to predict the correct pairings of images and texts.


_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>




