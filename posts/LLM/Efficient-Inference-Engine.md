[![Home](https://img.shields.io/badge/Home-Click%20Here-blue?style=flat&logo=homeadvisor&logoColor=white)](../../)

## [![GenAI](https://img.shields.io/badge/GenAI-Selected_Topics_in_Generative_AI-green?style=for-the-badge&logo=github)](../../main_page/GenAI)

# Efficient Inference Engine
![inference](../../images/Efficient-Inference.png)


## [![FlashAttention](https://img.shields.io/badge/FLASHATTENTION-Fast_and_Memory_Efficient_Exact_Attention_with_IO_Awareness-blue?style=for-the-badge&logo=github)](FlashAttention)
FlashAttention is a pivotal innovation that addresses one of the most fundamental bottlenecks in the Transformer architecture. Unlike architectural changes like MoE or GQA, FlashAttention is a groundbreaking implementation that doesn't change the mathematics of attention but radically alters how it's computed on the hardware.
</div>
