[![Home](https://img.shields.io/badge/Home-Click%20Here-blue?style=flat&logo=homeadvisor&logoColor=white)](../)


<br>

## [![MixedPrecision](https://img.shields.io/badge/Optimization_Series-Mixed_Precision_Training-blue?style=for-the-badge&logo=github)](../posts/MixedPrecision)

<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
Mixed precision training is a technique in deep learning where computations are performed using different numerical precisions—typically a mix of **16-bit floating point (FP16)** and **32-bit floating point (FP32)**—to accelerate training and reduce memory usage while maintaining model accuracy. 
<p></p>
_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>


## [![Quantization](https://img.shields.io/badge/Optimization_Series-Quantization-blue?style=for-the-badge&logo=github)](../posts/Quantization)

<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
With the proliferation of deep learning models in various applications, deploying these models on resource-constrained devices like mobile phones, embedded systems, and IoT devices has become essential. Quantization is a key technique that reduces the model size and computational requirements by converting floating-point numbers to lower-precision representations, such as integers.


This tutorial provides an in-depth exploration of quantizing machine learning models. We will delve into the mathematical underpinnings, practical implementations using PyTorch, and advanced topics like mixed precision quantization and layer fusion. By the end of this tutorial, you will have a comprehensive understanding of quantization techniques and how to apply them effectively to optimize your machine learning models.

<p></p>

_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>

