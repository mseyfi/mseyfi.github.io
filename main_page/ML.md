[![Home](https://img.shields.io/badge/Home-Click%20Here-blue?style=flat&logo=homeadvisor&logoColor=white)](../)


<br>

## [![MLE](https://img.shields.io/badge/MLE/MAP-Maximum_Likelihood_and_Maximum_Aposteriori_Estimation-blue?style=for-the-badge&logo=github)](../posts/MLE)

<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
We talk about Maximum Likelihood Estimation (MLE) and Maximum a Posteriori (MAP) with clear intuitions, mathematical derivations, and concrete examples from neural networks and computer vision.
<p></p>
</div>

## [![MixedPrecision](https://img.shields.io/badge/Optimization_Series-Mixed_Precision_Training-blue?style=for-the-badge&logo=github)](../posts/MixedPrecision)

<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
Mixed precision training is a technique in deep learning where computations are performed using different numerical precisions—typically a mix of <b>16-bit floating point (FP16)</b> and <b>32-bit floating point (FP32)</b>—to accelerate training and reduce memory usage while maintaining model accuracy. 
<p></p>
</div>


## [![Quantization](https://img.shields.io/badge/Optimization_Series-Quantization-blue?style=for-the-badge&logo=github)](../posts/Quantization)

<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
With the proliferation of deep learning models in various applications, deploying these models on resource-constrained devices like mobile phones, embedded systems, and IoT devices has become essential. Quantization is a key technique that reduces the model size and computational requirements by converting floating-point numbers to lower-precision representations, such as integers.


This tutorial provides an in-depth exploration of quantizing machine learning models. We will delve into the mathematical underpinnings, practical implementations using PyTorch, and advanced topics like mixed precision quantization and layer fusion. By the end of this tutorial, you will have a comprehensive understanding of quantization techniques and how to apply them effectively to optimize your machine learning models.

<p></p>

</div>

## [![RANSAC](https://img.shields.io/badge/RANSAC-Random_Sample_Consensus-blue?style=for-the-badge&logo=github)](../posts/RANSAC)

<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
<b>RANSAC (Random Sample Consensus)</b> is an iterative method for robust parameter estimation. It attempts to find a model that best fits the <b>inlier</b> data points while minimizing the effect of <b>outlier</b> points.

<p></p>

</div>

## [![FocalLoss](https://img.shields.io/badge/FocalLoss-Focal_Loss-blue?style=for-the-badge&logo=github)](../posts/FocalLoss)

<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
Focal Loss is a modified version of the standard cross-entropy loss, designed to address the class imbalance problem, especially in tasks like object detection (e.g. RetinaNet) or extremely imbalanced binary classification.
<p></p>

</div>

## [![Kmeans](https://img.shields.io/badge/KMEANS-Unsupervised_Learning-blue?style=for-the-badge&logo=github)](../posts/Kmeans)

<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
K-Means is a popular clustering algorithm used to partition a dataset into <b>K clusters</b> by minimizing intra-cluster variance. A crucial factor in its performance is how you initialize the cluster centroids. 
<p></p>
</div>

## [![Ensemble](https://img.shields.io/badge/DecisionTrees-Ensemble_Methods_Bagging_and_Boosting-blue?style=for-the-badge&logo=github)](../posts/Ensemble)
<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
Ensemble Learning: Bootstrap Sampling, Bagging (Random Forest), Boosting (AdaBoost, Gradient Boosting, XGBoost).
<p></p>
</div>

## [![DecisionTrees](https://img.shields.io/badge/DecisionTrees-Rule_Based_Model-blue?style=for-the-badge&logo=github)](../posts/DecisionTrees)
<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
A Decision Tree is a recursive, rule-based model that partitions the feature space R^n into disjoint regions and assigns a prediction to each region. It works by splitting the dataset at each node based on feature values to reduce some measure of impurity or error.
<p></p>
</div>

## [![SVM](https://img.shields.io/badge/SVM-Support_Vector_Machines-blue?style=for-the-badge&logo=github)](../posts/SVM)
<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
Support Vector Machines, Hard Margin SVM, Soft Margin SVM, Kernel Tricks, and Support Vector Regression
<p></p>
</div>
