
<div style="background-color: #f3f3f3; color: #512; font-weight: 500; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
  <h2>Preface:</h2>
  <p>
Hi, Iâ€™m Mehdi, and I use this blog to document my learning journey.
I am a research scientist at Meta <img src="images/Meta.png"  width="30" height="30"> | Exploring computer vision, computational photography, and Gen-AI ðŸš€.

Iâ€™ve discovered that I grasp concepts more effectively when I understand a simple code example associated with them. Therefore, in this blog, I explore new concepts through coding!|

I try to use simple scenarios and of course, I never use ChatGPTðŸ¤¥.
</p>
</div>


[<img src="images/LinkedIn_logo_initials.png"  width="40" height="40">](https://www.linkedin.com/in/mehdi-seyfi-38189220/)   [<img src="images/googlescholar.png"  width="40" height="40">](https://scholar.google.ca/citations?user=6l0PmOEAAAAJ&hl=en) [<img src="images/github.png"  width="40" height="40">](https://github.com/mseyfi)
<br>
<br>
<br>
<br>
<br>
<br>



[<img src="images/GenAI.png"  width="150" height="150">](main_page/GenAI) | [<img src="images/CV.png"  width="150" height="150">](main_page/CV)
|:--:|:--:| 
**Generative AI**|**Computer Vision**|

[<img src="images/ML.png"  width="150" height="150">](main_page/ML) | [<img src="images/CP.png"  width="150" height="150">](main_page/CP)
|:--:|:--:| 
**Machine Learning**|**Computational Photography**|

## [![GenAI](https://img.shields.io/badge/GenAI-Selected_Topics_in_GenAI-green?style=for-the-badge&logo=github)](main_page/GenAI)
## [![CV](https://img.shields.io/badge/CV-Selected_Topics_in_Computer_Vision-green?style=for-the-badge&logo=github)](main_page/CV)
## [![CP](https://img.shields.io/badge/CP-Selected_Topics_in_Computational_Photography-green?style=for-the-badge&logo=github)](main_page/CP)
## [![ML](https://img.shields.io/badge/ML-Selected_Topics_in_Machine_Learning-green?style=for-the-badge&logo=github)](main_page/ML)



<p></p>
<p></p>


## [![ControlNet](https://img.shields.io/badge/ControlNet-Adding_Conditional_Control_to_Text_to_Image_Diffusion_Models-blue?style=for-the-badge&logo=github)](posts/ControlNet)
<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
ControlNet is an advanced extension of diffusion models that introduces additional control mechanisms, allowing for precise guidance over the generation process. By integrating control signals (e.g., edge maps, segmentation masks, poses), ControlNet enables the generation of images that adhere to specific structural or semantic constraints provided by the user.
<p></p>
_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>

## [![CFG](https://img.shields.io/badge/CFD-Classifier_Free_Diffusion-blue?style=for-the-badge&logo=github)](posts/CFG)
<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
Classifier-Free Diffusion is a powerful technique in generative modeling, particularly within diffusion models, that enhances the quality and controllability of generated outputs without relying on an external classifier. This comprehensive guide will delve into the intricacies of classifier-free diffusion, covering its principles, training and inference processes, intuitive explanations, and practical implementations in tasks like image inpainting, super-resolution, and text-to-image generation.
<p></p>
_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>


## [![Classifier-Guided Diffusion](https://img.shields.io/badge/CGD-Classifier_Guided_Diffusion-blue?style=for-the-badge&logo=github)](posts/Classifier_Guided_Diffusion)
<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
Classifier-guided diffusion is a powerful technique in generative modeling that leverages an external classifier to steer the generation process toward desired attributes or classes. This method enhances the quality and controllability of generated data, such as images, by integrating class-specific information during the diffusion process.
<p></p>
_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>

## [![Latent Diffusion](https://img.shields.io/badge/LDM-Latent_Diffusion_Models-blue?style=for-the-badge&logo=github)](posts/StableDiffusion)
<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
Stable Diffusion is a powerful generative model that synthesizes high-quality images guided by textual/another modality descriptions. It leverages the strengths of Variational Autoencoders (VAEs) and Denoising Diffusion Probabilistic Models (DDPMs) to produce images efficiently and effectively.
 <p></p>
_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>

## [![Diffusion Models DDPM](https://img.shields.io/badge/DDPM-Diffusion_Models-blue?style=for-the-badge&logo=github)](posts/Diffusion)
<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
In this guide, we'll provide sample code for training and inference of a diffusion model, specifically focusing on a Denoising Diffusion Probabilistic Model (DDPM). We'll define the structure for the encoder and decoder using a simplified UNet architecture. Each line of code includes inline comments explaining its purpose, along with the tensor shapes.
 <p></p>
_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>

## [![GLIP](https://img.shields.io/badge/GLIP-Grounded_Language_Image_Pre_training-blue?style=for-the-badge&logo=github)](posts/GLIP)
<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
GLIP (Grounded Language-Image Pre-training) is a unified model architecture that bridges the gap between vision and language by integrating object detection and phrase grounding tasks. It leverages both visual and textual data to perform object detection conditioned on textual descriptions, enabling the model to recognize objects based on their semantic meanings.
<p></p>
_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>

## [![CLIP](https://img.shields.io/badge/CLIP-Learning_Transferable_Visual_Models_From_Natural_Language_Supervision-blue?style=for-the-badge&logo=github)](posts/CLIP)
<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
Learning Transferable Visual Models From Natural Language Supervision" is a groundbreaking paper by OpenAI that introduces CLIP (Contrastive Language-Image Pre-training). CLIP learns visual concepts from natural language supervision by jointly training an image encoder and a text encoder to predict the correct pairings of images and texts.
<p></p>
_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>

## [![texttoimage](https://img.shields.io/badge/Text_to_Image-grey?style=for-the-badge&logo=github)](posts/TextToImage)
<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
notable text-to-image generation models along with their corresponding research papers, sorted by the year they were published:
 <p></p>
_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>

## [![DETR](https://img.shields.io/badge/DETR-Detection_Transformer-blue?style=for-the-badge&logo=github)](posts/DETR)
<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
The Detection Transformer (DETR) is a novel approach to object detection that leverages Transformers, which were originally designed for sequence-to-sequence tasks like machine translation. Introduced by Carion et al. in 2020, DETR simplifies the object detection pipeline by eliminating the need for hand-crafted components like anchor generation and non-maximum suppression (NMS).
 <p></p>
_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>

## [![VIT](https://img.shields.io/badge/VIT-Vision_Transformers-blue?style=for-the-badge&logo=github)](posts/VIT)
<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
Vision Transformers (ViTs) apply the Transformer architecture, originally designed for natural language processing (NLP), to computer vision tasks like image classification. ViTs treat an image as a sequence of patches (akin to words in a sentence) and process them using Transformer encoders. <p></p>
_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>

## [![CGANS](https://img.shields.io/badge/CGANs-Conditional_GAN-blue?style=for-the-badge&logo=github)](posts/ConditionalGan)
<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
Python implementation of a Conditional Generative Adversarial Network (cGAN) using PyTorch.
 <p></p>
_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>

## [![Distillation](https://img.shields.io/badge/Distillation-grey?style=for-the-badge&logo=github)](posts/Distillation)
<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
Generalization issue with Distillation
 <p></p>
_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>

## [![MixedPrecision](https://img.shields.io/badge/Optimization_Series-Mixed_Precision_Training-blue?style=for-the-badge&logo=github)](posts/MixedPrecision)
<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
Mixed precision training is a technique in deep learning where computations are performed using different numerical precisionsâ€”typically a mix of **16-bit floating point (FP16)** and **32-bit floating point (FP32)**â€”to accelerate training and reduce memory usage while maintaining model accuracy. 
<p></p>
_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>

## [![Quantization](https://img.shields.io/badge/Optimization_Series-Quantization-blue?style=for-the-badge&logo=github)](posts/Quantization)
<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
With the proliferation of deep learning models in various applications, deploying these models on resource-constrained devices like mobile phones, embedded systems, and IoT devices has become essential. Quantization is a key technique that reduces the model size and computational requirements by converting floating-point numbers to lower-precision representations, such as integers.

This tutorial provides an in-depth exploration of quantizing machine learning models. We will delve into the mathematical underpinnings, practical implementations using PyTorch, and advanced topics like mixed precision quantization and layer fusion. By the end of this tutorial, you will have a comprehensive understanding of quantization techniques and how to apply them effectively to optimize your machine learning models.
<p></p>
_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>



## [![MC](https://img.shields.io/badge/MC-Motion_Compensation-blue?style=for-the-badge&logo=github)](posts/MotionCompensation)
<div style="background-color: #f0f8ff; color: #555;font-weight: 485; padding: 20px; margin: 20px 0; border-radius: 8px; border: 1px solid #ccc;">
Motion estimation and motion compensation are critical components in video compression algorithms. They exploit temporal redundancies between consecutive frames in a video sequence to reduce the amount of data required for efficient storage and transmission. By predicting the motion of objects from one frame to another, we can represent a video more compactly without significantly compromising visual quality.
<p></p>
_Last updated: {{ site.time | date: "%B %d, %Y" }}_
</div>


